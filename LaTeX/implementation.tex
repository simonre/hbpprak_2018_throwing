\section{Implementation}

\subsection{Technical details}

Since evolutionary algorithms played a central role in most of the approaches presented in the following sections, a small module was implemented in Python. The module accepts an initial \textit{numpy} array, clones it and randomly mutates each clone to generate the starting population. It also offers functions to set the fitness of each individual in the last generation and uses these fitness values to generate the next generation when requested. Available parameters are the pool size, the mutation rate and the mating pool size.

\begin{figure}
	\begin{center}
		\begin{tikzpicture}[>=stealth',shorten >=1pt,auto]
			\node[rectangle,draw,minimum size=1cm] (EM)                   {$Genetic\ module$};
			\node[rectangle,draw,minimum size=1cm] (VC) [right=1cm of EM] {$Virtualcoach$};
			\node[rectangle,draw,minimum size=1cm] (NRP)[right=1cm of VC] {$NRP$};

			\path[->]	(EM)  edge	[bend left=45] node {movement} (VC)
						(VC)  edge	[bend left=45] node	{transfer function} (NRP)
						(NRP) edge	[bend left=45] node {fitness} (VC)
						(VC)  edge	[bend left=45] node {fitness} (EM);
		\end{tikzpicture}
	\end{center}
	\caption{The interactions between the genetic module, the virtualcoach and the NRP simulation.}
	\label{fig:virtualcoach}
\end{figure}

The control of the execution inside the NRP simulation is handled by the \textit{virtualcoach} as seen in Figure \ref{virtualcoach}. For each individual in the latest generation a new transfer function is generated. When the state machine transitions into a new state, this change is published to a ROS topic, triggering the listening transfer function, which sends the corresponding movements to the robot arm. After the simulation, the measured distance of the cylinder is used as the fitness value of the individual and stored in the genetic module. The \textit{virtualcoach} then sets the transfer function for the next individual and restarts the simulation.


\subsection{Prepare and Hit} \label{sec:PrepareHit}

The first approach is to let the robot arm hit the cylinder away from the table. Therefore, the movement is divided into three phases: reset, prepare, and hit. The phases are implemented as states using the state machine editor.
Figure \ref{fig:RPH} shows the sequence of the three states.

\begin{figure}
	\begin{center}
		\begin{tikzpicture}[>=stealth',shorten >=1pt,auto]
			\node[initial,state,minimum size=1.6cm] (Reset)   								{$Reset$};
			\node[state,minimum size=1.6cm]         (Prepare)	[right=1cm of Reset] 		{$Prepare$};
			\node[state,minimum size=1.6cm]         (Hit)	  	[right=1cm of Prepare]		{$Hit$};

			\path[->]	(Reset)		edge	node 				{} (Prepare)
						(Prepare)	edge	node				{} (Hit)
						(Hit)		edge	[bend left=45] node {} (Reset);
		\end{tikzpicture}
	\end{center}
	\caption{The three states reset, prepare, and hit are used to let the robot arm hit the cylinder away from the table.}
	\label{fig:RPH}
\end{figure}

\subsubsection{Hard-coded movements}

At first, the movements are hard-coded:

\begin{itemize}
\item Reset: Moves the arm in an upright position and puts the cylinder on the top right corner of the table (as seen from the robot arm).
\item Prepare: Moves the arm in an horizontal position next to the cylinder (on the left side of the cylinder as seen from the robot arm).
\item Hit: The arm moves towards the cylinder and knocks it off the table.
\end{itemize}

\subsubsection{Hit movement improved} 

This approach also has the aim of knocking the cylinder off the table, however, the hit movement is not hard-coded but learned using an evolutionary algorithm. The reset and prepare phases are taken from the hard-coded approach.
The evolutionary algorithm which is used to let the robot learn the hit movement is described below: 

An individual of each generation consists of an array of six components (genes) that represent the joints of the robot. The start array is defined as $\begin{bmatrix}
				1 & 1 & 1 & 1 & 1 & 1
				\end{bmatrix}$.
To generate the initial population of ten individuals the start array is mutated by adding random values between $\{-0.1,0.1\}$ to each component.
Computing the next generation consists of three steps:
\begin{enumerate}
\item Selection: The mating pool of four parents is created by selecting the individuals with the highest fitness. The fitness is calculated using the difference between the initial and the end position of the cylinder after knocking it off the table.

\item Mating: In this algorithm, the parents are carried over to the next generation. The recombination process uses one-point crossovers. 

\item Mutation: The offspring is mutated by adding a randomly calculated value between $\{-0.5,0.5\}$ to each component.
\end{enumerate}

These steps are repeated and after ? generations the winner is selected.

\subsubsection{Prepare and Hit movement improved} 

This approach also uses an evolutionary approach to knock the cylinder off the table. In this approach both the prepare and the hit movement are learned. The reset movement is taken from the hard-coded approach. The evolutionary algorithm is the same as in the previous subsection but with a different start array. 

The start array is a combination of the prepare and the hit movement. It is  created by appending the hit array, given as  
$\begin{bmatrix}
-0.45 &  -0.9 & 0.9 & 0 & 0 & -0.5
\end{bmatrix}$ 
to the prepare array, also given as
$\begin{bmatrix}
-0.45 &  -0.9 & 0.9 & 0 & 0 & -0.5
\end{bmatrix}$.
In this approach, the start pool consists of 15 individuals which are generated from the start array by adding random values between $\{-0.25,0.25\}$ to each component. The mating pool consists of three parents.
The evolutionary algorithm is executed as in the previous approach (with the described changes).

It is repeated for eight generations. 


\subsection{Prepare, Grasp, and Throw} \label{sec:Throw}

The second approach uses grasp and throw phases to throw the cylinder away from the table. The reset and prepare phases are taken from the first approach and the hit phase is replaced by grasp and throw phases.
Figure \ref{fig:RPGT} shows the sequence of the four phases.

\begin{figure}
	\begin{center}
		\begin{tikzpicture}[>=stealth',shorten >=1pt,auto]
			\node[initial,state,minimum size=1.6cm] (Reset)									{$Reset$};
			\node[state,minimum size=1.6cm]         (Prepare) 	[right=1cm of Reset]		{$Prepare$};
			\node[state,minimum size=1.6cm]         (Grasp) 	[right=1cm of Prepare]		{$Grasp$};
			\node[state,minimum size=1.6cm]         (Throw) 	at (2.65,-2)				{$Throw$};

			\path[->]	(Reset)		edge	node {} (Prepare)
						(Prepare)	edge	node {} (Grasp)
						(Grasp)		edge	node {} (Throw)
						(Throw)		edge	node {} (Reset);

		\end{tikzpicture}
	\end{center}
	\caption{The four states reset, prepare, grasp, and throw are used to let the robot arm throw the cylinder away from the table.}
	\label{fig:RPGT}
\end{figure}

\subsubsection{Hard-coded movements}

At first, the movements are hard-coded.

\begin{itemize}
\item Reset: Moves the arm in an upright position and puts the cylinder on the top right corner of the table (as seen from the robot arm).
\item Prepare: Moves the arm next to the cylinder (on the left as seen from the robot arm).
\item Grasp: Moves the fingers of the robot hand around the cylinder in order to grab it.
\item Throw: With high acceleration, the arm moves backwards beyond the vertical position and the hand opens to throw the cylinder away.
\end{itemize}


\subsubsection{Throw movement improved} 

This section describes two different ideas to optimize the throw movement. In both approaches the aim is to let the robot learn which movement is best to throw the cylinder as far away from the table as possible.

\subsubsection{Evolutionary Approach} \label{subsec:EA}

This approach uses the phases from the second approach. The reset, prepare, and grasp are still hard-coded as in the second approach. The throw phase is trained using an evolutionary algorithm. 


\subsection{Markov Chain Monte Carlo Approach} \label{subsec:MCMC}

This approach also uses the phases from the fourth approach. The reset, prepare, and grasp are still hard-coded as in the second approach. The throw phase is trained using a Markov Chain Monte Carlo algorithm. 


\subsection{Prepare, Grasp, Windup, and Throw} \label{sec:Windup}

Figure \ref{fig:RPGWT} shows the state sequence using an additional windup phase.

\begin{figure}
	\begin{center}
		\begin{tikzpicture}[>=stealth',shorten >=1pt,auto]
			\node[initial,state,minimum size=1.6cm] (Reset)								{$Reset$};
			\node[state,minimum size=1.6cm]         (Prepare) 	[right=1cm of Reset]	{$Prepare$};
			\node[state,minimum size=1.6cm]         (Grasp) 	[right=1cm of Prepare]	{$Grasp$};
			\node[state,minimum size=1.6cm]         (Windup) 	at (4,-2) 				{$Windup$};
			\node[state,minimum size=1.6cm]         (Throw) 	at (1.3,-2)				{$Throw$};

			\path[->]	(Reset)		edge	node {} (Prepare)
						(Prepare)	edge	node {} (Grasp)
						(Grasp)		edge	node {} (Windup)
						(Windup)	edge	node {} (Throw)
						(Throw)		edge	node {} (Reset);

		\end{tikzpicture}
	\end{center}
	\caption{reset, prepare, grasp, windup, throw}
	\label{fig:RPGWT}
\end{figure}
