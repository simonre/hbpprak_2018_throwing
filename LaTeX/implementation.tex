\section{Implementation}

\subsection{Technical details}

Since evolutionary algorithms played a central role in most of the approaches presented in the following sections, a small module was implemented in Python. The module accepts an initial \textit{numpy} array, clones it and randomly mutates each clone to generate the starting population. It also offers functions to set the fitness of each individual in the last generation and uses these fitness values to generate the next generation when requested. Available parameters are the pool size, the mutation rate and the mating pool size.

\begin{figure}
	\begin{center}
		\begin{tikzpicture}[>=stealth',shorten >=1pt,auto]
			\node[rectangle,draw,minimum size=1cm] (EM)                   {$Genetic\ module$};
			\node[rectangle,draw,minimum size=1cm] (VC) [right=1cm of EM] {$Virtualcoach$};
			\node[rectangle,draw,minimum size=1cm] (NRP)[right=1cm of VC] {$NRP$};

			\path[->]	(EM)  edge	[bend left=45] node {movement} (VC)
						(VC)  edge	[bend left=45] node	{transfer function} (NRP)
						(NRP) edge	[bend left=45] node {fitness} (VC)
						(VC)  edge	[bend left=45] node {fitness} (EM);
		\end{tikzpicture}
	\end{center}
	\caption{The interactions between the genetic module, the virtualcoach and the NRP simulation.}
	\label{fig:virtualcoach}
\end{figure}

The control of the execution inside the NRP simulation is managed by the \textit{virtualcoach} as seen in Figure \ref{virtualcoach}. The simulation runs in a continuous loop, repeating the same movements according to the state machine defined in each approach. \todo{Describe better:} To synchronize with the virtualcoach, the simulation will publish status updates that carry information about the current simulation state.This allows the virtualcoach to pause the simulation after the simulation has run once, extract th encessary information, update the transfer functions and resume the execution.

The tasks carried out during the learning process are the following:
\begin{enumerate}
\item For each individual in the latest generation a new transfer function is generated. When the state machine transitions into a new state, this change is published to a ROS topic, the listening transfer function, which sends the corresponding joint forces to the robot arm.
\item The simulation runs the requested robot arm movements.
\item The simulation registers the new position of the cylinder and calculates the traveled distance.
\item The virtualcoach detects that the simulation is restarting, pauses it, reads out the traveled distance of the cylinder and sets it as the fitness of this individual. It then proceeds to update the trasnfer function to simulate the next individual and resumes the simulation.
\end{enumerate}

This process continues until the required number of generations have been simulated. The winning movement is the fittest of the last generation.


\subsection{Prepare and Hit} \label{sec:PrepareHit}

The first approach is to let the robot arm hit the cylinder away from the table. Therefore, the movement is divided into three phases: reset, prepare, and hit. The phases are implemented as states using the state machine editor.
Figure \ref{fig:RPH} shows the sequence of the three states.

\begin{figure}
	\begin{center}
		\begin{tikzpicture}[>=stealth',shorten >=1pt,auto]
			\node[initial,state,minimum size=1.6cm] (Reset)   								{$Reset$};
			\node[state,minimum size=1.6cm]         (Prepare)	[right=1cm of Reset] 		{$Prepare$};
			\node[state,minimum size=1.6cm]         (Hit)	  	[right=1cm of Prepare]		{$Hit$};

			\path[->]	(Reset)		edge	node 				{} (Prepare)
						(Prepare)	edge	node				{} (Hit)
						(Hit)		edge	[bend left=45] node {} (Reset);
		\end{tikzpicture}
	\end{center}
	\caption{The three states reset, prepare, and hit are used to let the robot arm hit the cylinder away from the table.}
	\label{fig:RPH}
\end{figure}

\subsubsection{Hard-coded movements} \label{sec:prepare_hit_hard}

To get a feeling of how the robot arm could hit the cylinder, the first attempt involved hard-coding the movements. In each state, the following actions should be simulated:

\begin{itemize}
\item Reset: Move the arm in an upright position and reset the cylinder position on the table.
\item Prepare: Move the arm to a horizontal position, the hands ends up next to the cylinder.
\item Hit: The arm moves towards the cylinder and knocks it off the table.
\end{itemize}

\todo{The resulting movements was
$\begin{bmatrix}
1 & 1 & 1 & 1 & 1 & 1
\end{bmatrix}$ ???}

\subsubsection{Learning the Hit movement} \label{sec:prepare_hit_learn_hit}

In this next approach, the hit movement is not hard-coded but learned using an evolutionary algorithm. The reset and prepare phases are taken from the hard-coded approach \ref{sec:prepare_hit_hard}. An individual consists of an array of six components that represent the force to apply to each joint of the robot. The start array used was the movement acquired from the hard-coded approach in \ref{sec:prepare_hit_hard}.

The parameters used for the evolutionary learning are described in the following: 

\begin{itemize}
\item \textbf{Pool size:} The pool size was fixed at \textbf{15} individuals in each generation. \todo{Describe that more would not be feasible?}
\item \textbf{Mutation rate:} To leave more space to early exploration without restricting the possible variations of the initial movement, a relatively high mutation rate of \todo{\textbf{5}} was first selected, but is decreased by \todo{\textbf{20\%}} in each new generation to allow for more fine-tuning later on. This initial mutation rate is also applied to mutate the first generation that is created from the start vector.
\item \textbf{Mating pool size:} The mating pool size was \todo{\textbf{4}}. These 4 fittest individuals are also carried on to the next generation, giving the algorithm a chance to recover even if the offspring was exceptionally bad at the task or a glitch in the simulation resulted in an extremely high fitness.
\item \textbf{Crossover:} One-point-crossover was used to combine the fittest individuals to produce the next generation.
\end{itemize}

The number of simulated generations was \textbf{6}, after which the robot arm was able o not just knock the cylinder from the table, but hit it significantly further away.


\subsubsection{Learning the Prepare and Hit movements} 

On top of the approach described in \ref{sec:prepare_hit_learn_hit}, this approach attempts to also learn an improved version of the prepare movement. The intuition was that a preparation movement that begins the swing from a greater distance from the cylinder would result in more momentum and therefore a stronger hit.
The robot joint force vectors corresponding to the prepare and hit movements are simply concatenated, resulting in gene of 12 elements.
\todo{Include values?
$\begin{bmatrix}
-0.45 &  -0.9 & 0.9 & 0 & 0 & -0.5
\end{bmatrix}$ 
to the prepare array, also given as
$\begin{bmatrix}
-0.45 &  -0.9 & 0.9 & 0 & 0 & -0.5
\end{bmatrix}$.}

\begin{itemize}
\item \textbf{Pool size:} As in \ref{sec:prepare_hit_learn_hit}, the pool size was \textbf{15}. \todo{Describe that more would not be feasible?}
\item \textbf{Mutation rate:} As in \ref{sec:prepare_hit_learn_hit}, the mutation rate was \todo{\textbf{5}}.
\item \textbf{Mating pool size:} As in \ref{sec:prepare_hit_learn_hit}, the mating pool size was \todo{\textbf{4}}. These 4 fittest individuals were still carried on to the next generation.
\item \textbf{Crossover:} Sinve the first half of the gene is now exclusive to the prepare movement and the second helf to the hit movement, performing a one-point-crossover at the center would not be appropriate. Therefore, n-point-crossover was used, where for each component of the offspring vector there is a 50\% likelihood to inherit the value from the second parent instead of the first.
\end{itemize}

After 6 generations, the fittest simulated prepare movement started the swing further away from the cylinder than the previous hardcoded versions. However, the improvement was less than expected, possibly because the chance of getting good both a good prepare movement and a good hit movement simultaneously during the mutation is very low. Also, what might be a good prepare movement in combination with one specific hit action might lead to drastically different results when paired with a different hit motion.


\subsection{Prepare, Grasp, and Throw} \label{sec:Throw}

The second approach attempts to mimic humans by first grasping the cylinder and then throwing it.
The reset and prepare phases remain the same as in \ref{sec:PrepareHit}, only the hit phase is replaced by the grasp and throw phases.
Figure \ref{fig:RPGT} shows the sequence of the four phases.

\begin{figure}
	\begin{center}
		\begin{tikzpicture}[>=stealth',shorten >=1pt,auto]
			\node[initial,state,minimum size=1.6cm] (Reset)									{$Reset$};
			\node[state,minimum size=1.6cm]         (Prepare) 	[right=1cm of Reset]		{$Prepare$};
			\node[state,minimum size=1.6cm]         (Grasp) 	[right=1cm of Prepare]		{$Grasp$};
			\node[state,minimum size=1.6cm]         (Throw) 	at (2.65,-2)				{$Throw$};

			\path[->]	(Reset)		edge	node {} (Prepare)
						(Prepare)	edge	node {} (Grasp)
						(Grasp)		edge	node {} (Throw)
						(Throw)		edge	node {} (Reset);

		\end{tikzpicture}
	\end{center}
	\caption{The four states reset, prepare, grasp, and throw are used to let the robot arm throw the cylinder away from the table.}
	\label{fig:RPGT}
\end{figure}

\subsubsection{Hard-coded movements}

As in \ref{sec:prepare_hit_hard}, the first attempt was to hard code the movements. In the two new states, the robot arm should:

\begin{itemize}
\item Grasp: Close the fingers of the robot hand around the cylinder, grabbing it.
\item Throw: With high acceleration, move backwards beyond the vertical position and the open the hand to release the cylinder and throw it away.
\end{itemize}

\todo{Describe non-deterministic grabbing?}

\subsubsection{Learning the Throw movement} \label{subsec:EA}

In this approach, the robot should learn to improve the throw movement. For this, the same evolutionary method as in \ref{sec:PrepareHit} was used.
\todo{parameters?}

The results were fascinating, but logical. The robot arm learned to spin extremely fast around it's base, propelling the cylinder far over the edge of the scenario. While not good for the mechanical components of the robot in the real world, this development made certainly sense inside the simulation.


\subsection{Markov Chain Monte Carlo Approach} \label{subsec:MCMC}

\todo{}
This approach also uses the phases from the fourth approach. The reset, prepare, and grasp are still hard-coded as in the second approach. The throw phase is trained using a Markov Chain Monte Carlo algorithm. 


\subsection{Prepare, Grasp, Windup, and Throw} \label{sec:Windup}

\todo{}
Figure \ref{fig:RPGWT} shows the state sequence using an additional windup phase.

\begin{figure}
	\begin{center}
		\begin{tikzpicture}[>=stealth',shorten >=1pt,auto]
			\node[initial,state,minimum size=1.6cm] (Reset)								{$Reset$};
			\node[state,minimum size=1.6cm]         (Prepare) 	[right=1cm of Reset]	{$Prepare$};
			\node[state,minimum size=1.6cm]         (Grasp) 	[right=1cm of Prepare]	{$Grasp$};
			\node[state,minimum size=1.6cm]         (Windup) 	at (4,-2) 				{$Windup$};
			\node[state,minimum size=1.6cm]         (Throw) 	at (1.3,-2)				{$Throw$};

			\path[->]	(Reset)		edge	node {} (Prepare)
						(Prepare)	edge	node {} (Grasp)
						(Grasp)		edge	node {} (Windup)
						(Windup)	edge	node {} (Throw)
						(Throw)		edge	node {} (Reset);

		\end{tikzpicture}
	\end{center}
	\caption{reset, prepare, grasp, windup, throw}
	\label{fig:RPGWT}
\end{figure}
