\section{Initial thoughts} \label{sec:motivation}

There are some questions that every learning approach should take into consideration. One typical question in reinforcement learning problems is the trade-off between exploration and exploitation. In this particular problem, this trade-off manifests itself in the trade-off between continuing to explore random movements and exploiting movements that the reward function has deemed as good. One of the ways this can be influenced is by varying the learning rate.

Another problem we quickly ran into with this task was that executing the movements took a very long time, which limited the number of samples we could get from the simulation. Therefore, the learning mechanism we chose had to be quick and efficient. This rules out artificial neural networks and spiking neural networks.

In summary, these questions had to be answered by every approach to this problem:

\begin{itemize}
	\item \textbf{What is an appropriate learning rate?} We soon discovered that a learning rate that was too high led to unpredictable movements of the robot and steered the robot arm away from movements that would actually be good, whereas a learning rate that was too small would not produce any satisfactory results as the arm would not get enough training.
	\item \textbf{What is a good algorithm choice?} We needed an algorithm that is both robust to some outliers as well as being reasonably fast.
\end{itemize}
